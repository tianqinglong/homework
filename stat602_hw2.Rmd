---
title: "stat602_hw2"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyr)
library(caret)
library(readxl)
library(glmnet)
library(lava)
```

# 4.3

## First we read in the data set and remove the irrelavant columns.
```{r}
house <- read_excel("~/Downloads/AmesHousingData.xlsx")
house <- data.frame(Price = house$Price, Size = house$Size,
                    Fireplace = house$Fireplace,
                    Basementbath = house$`Bsmt Bath`, Land = house$Land, intersect = 1)
head(house)
```

## Now we need a matrix of all the possible regressor combinations.
```{r}
regMat <- expand.grid(c(TRUE,FALSE), c(TRUE,FALSE),
                      c(TRUE,FALSE), c(TRUE,FALSE))
regMat <- cbind(TRUE,regMat)

head(regMat)
```

## Then we construct a formula so that from each row of regMat we can determine the the regressor combinations.
```{r}
formu <- function(vec)
{
  vec <- as.matrix(vec)
  regressors <- c("intersect","Size", "Fireplace", "Basementbath", "Land")
  out <- as.formula(paste(c("Price ~ 0", regressors[vec]), collapse=" + "))
  
  return(out)
}

formu(regMat[1,])
```

## Cross-validation using **caret** package

We will repeat the 8-fold crossvalidation 10 times.
```{r, warning=FALSE}
ctrl <- trainControl(method = "repeatedcv", repeats = 5, number = 8)

RMSE <- numeric(16)
for(i in 1:length(RMSE)){
  model_caret <- train(formu(regMat[i,]), data = house, trControl = ctrl, method = "lm")
  RMSE[i] = model_caret$results[,2]
}
RMSE
order(RMSE)
```
We can see that the full model has the smallest mean squared prediction error.

# 4.4

First read in the dataset:
```{r}
glass <- read.csv("~/work/homework/glass.data", header=FALSE)
names(glass) <- c("Id", "RI", "Na", "Mg", "Al", "Si", "K", "Ca", "Ba", "Fe", "Type")
glass_sub <- subset(glass, Type %in% c(1,2))
glass_sub$Type <- as.factor(glass_sub$Type)
```

## (a)

The best number of neighbors for this prediction task is $1$.
```{r}
ctrl <- trainControl(method="repeatedcv",repeats = 10, number = 10)
knnFit <- train(Type ~ . - Id, data = glass_sub, method = "knn", trControl = ctrl, preProcess = c("center","scale"), tuneGrid = expand.grid(k = 1:20) , tuneLength = 30)
plot(knnFit)
```

## (b)

See hand-written solution.

# 5.1

## (a)
```{r}
X <- c(2, 4, 3, 5, 1, 4, 3, 4, 2, 3, 7, 5, 6, 4, 4, 2, 5, 1, 2, 4)
X <- matrix(X, nrow = 5)

qrObj <- qr(X)
svdObj <- svd(X)
```

The Q matrix and R matrix are:
```{r}
qr.Q(qrObj)
qr.R(qrObj)
```
The colums of Q matrix give the bases for $C(\textbf{X})$.

The SVD results are:
```{r}
svdObj
```
The U has columns spanning $C(\textbf{X})$.

## (b)

The eigen decomposition of $X^{'}X$ isï¼š
```{r}
(V <- svdObj$v)
(D_sq <- diag(svdObj$d) %*% diag(svdObj$d))
```

The eigen decomposition of $XX^{'}$ is:
```{r}
(U <- svdObj$u)
(D_sq <- diag(svdObj$d) %*% diag(svdObj$d))
```

## (c)

rank = 1
```{r}
X %*% svdObj$v[,1]
```

rank = 2
```{r}
X %*% svdObj$v[,c(1,2)]
```

## (d)

First center the matrix:
```{r}
X_center <- scale(X, center = TRUE, scale = FALSE)

X_center_scale <- scale(X, center = T, scale = T)
```

```{r}
( svdObj2 <- svd(X_center) )
( svdObj3 <- svd(X_center_scale) )
```

The principal component directions:
```{r}
svdObj2$v
svdObj3$v
```

The "loadings" of the first principal component:
```{r}
svdObj2$v[,1]
svdObj3$v[,1]
```

The principal components:
```{r}
X_center %*% svdObj2$v
X_center_scale %*% svdObj3$v
```

## (e)

The rank = 1 approximation:
```{r}
X_center %*% svdObj2$v[,1]
X_center_scale %*% svdObj3$v[,1]
```

The rank = 2 approximation:
```{r}
X_center %*% svdObj2$v[,c(1:2)]
X_center_scale %*% svdObj3$v[,c(1:2)]
```

## (f)

```{r}
( eigenX <- eigen( 1/5 * t(X_center) %*% X_center ) )
1/5 * t(X_center) %*% X_center %*% eigenX$vectors[,c(1,2)]

( eigenX2 <- eigen( 1/5 * t(X_center_scale) %*% X_center_scale ) )
1/5 * t(X_center_scale) %*% X_center_scale %*% eigenX2$vectors[,c(1,2)]
```

# Problem 2

```{r, include= FALSE}
data2 <- read_excel("~/work/homework/Problem3.4Data.xlsx")
```

Making up a matrix:
```{r}
mother_wavelet <- function(x){
  
  value = ( x > 0 && x <= 1/2 ) - ( x > 1/2 && x <= 1)
  
  return(value)
}

haar_basis <- function(x){
  index <- 1;
  row <- numeric(16)
  for(m in 0:3)
    for( j in 0:( 2^m - 1 ) ){
      row[index] = sqrt(2^m) * mother_wavelet( (2^m) * ( x - j / (2^m) ) )
      index = index + 1
    }
  
  return(row)
}

x_trans <- ( data2$x - min(data2$x) )/( max(data2$x) - min(data2$x) )
X_h <- t( apply(as.matrix(x_trans), 1, haar_basis) )
```

## (a)

```{r}
( fit_ols <- lm(y~x, data = data2) )
plot(x = data2$x, y = data2$y, ylab = "Y", xlab = "X",
     pch = 1, cex = 0.5)
points(x = data2$x, y = fit_ols$fitted.values, pch = 16,
       col = "red", cex = 0.5)
legend("topleft", legend = c("Observed", "OLS Fitted"), col = c("black", "red"),
       pch = c(1, 16))
```

## (b)

```{r, warning = FALSE}
y_centered <- data2$y - mean( data2$y )

get_lambda <- function(M)
{
  fit_lasso <- glmnet(X_h, y_centered, standardize = T, pmax = M,nlambda = 20000)
  return( min(fit_lasso$lambda) )
}

get_predicted <- function(lambda)
{
  fit_lasso <- glmnet(X_h, y_centered, standardize = T, lambda = lambda)
  value = predict(fit_lasso, X_h)
  
  return(value)
}

get_coefficient_vector <- function(lambda){
  fit_lasso <- glmnet(X_h, y_centered, standardize = T, lambda = lambda)
  return(fit_lasso$beta)
}

M <- c(2,4,8)
lambda <- apply(as.matrix(M), 1, get_lambda)
value <- apply(as.matrix(lambda), 1, get_predicted) + mean(data2$y)

( coef <- apply( as.matrix(lambda), 1, get_coefficient_vector) )

plot(x = data2$x, y = data2$y, ylab = "Y", xlab = "X",
     pch = 1, cex = 0.5)

points(x = data2$x, y = value[,1], cex = 0.75, pch = 2, col = 2)
points(x = data2$x, y = value[,2], cex = 0.75, pch = 3, col = 3)
points(x = data2$x, y = value[,3], cex = 0.75, pch = 4, col = 4)

legend("topright", legend = c("M = 2", "M = 4", "M = 8"), col = c(2,3,4), pch = c(2,3,4))
```

# Problem 3

```{r, include= FALSE}
data2 <- read_excel("~/Documents/work/Problem3.4Data.xlsx")
```

```{r}
knot <- c(0, .1, .3, .5, .7, .9, 1)
K <- length(knot)

only_positive <- function(x){
  value <- ifelse(x >= 0, x, 0)
  
  return(value)
}

make_row <- function(x){
  row <- numeric(K)
  row[1] = 1
  row[2] = x
  
  for(j in 1:(K - 2)){
    value = only_positive( ( x - knot[j] )^3 )
    -( knot[K] - knot[j] )/( knot[K] - knot[K-1] ) * only_positive( (x - knot[K -1])^3 )
    +( knot[K-1] - knot[j] )/( knot[K] - knot[K-1] ) * only_positive( (x - knot[K])^3 )
    
    row[j+2] = value
  }
  
  return(row)
}

x_trans <- ( data2$x - min(data2$x) )/( max(data2$x) - min(data2$x) )
X_h_3 <- t( apply(as.matrix(x_trans), 1, make_row) )
```

Then to find $\hat{\beta}^{OLS}$:
```{r}
fit_ols <- lm(data2$y ~ 0 + X_h_3)
fit_ols$coefficients

x_plot <- seq(from = min( data2$x ), to = max( data2$x), length.out = 500)
x_plot_trans <- ( x_plot - min(data2$x) )/( max(data2$x) - min(data2$x) )
x_plot_h <- t( apply(as.matrix(x_plot_trans), 1, make_row) )
y_plot <- x_plot_h %*% fit_ols$coefficients

plot(x = data2$x, y = data2$y, pch = 16, cex = 0.5,
     ylab = "Y", xlab = "X", main = "Natural Cubic Regression Spline")
lines(x = x_plot, y = y_plot, col = "red")
```

# Problem 4

```{r, include=FALSE}
x <- seq(0,1,by=0.1)
N <- length(x)

compute_omiga <- function(j,k)
{
  value = 6 * (x[N-1] - x[k])^2 * (2*x[N-1] + x[k] - 3*x[j]) +
    12 * (x[N-1] - x[k]) * (x[N-1] - x[j]) *(x[N] - x[N-1])
  
  return(value)
}

compute_basis <- function(x_new, j){
  if(j == 1){
    value = 1
  }else if(j == 2){
    value = x_new
  }else{
    j = j-2
    value = only_positive( (x_new - x[j])^3 ) -
      ( x[N] - x[j] )/( x[N] - x[N-1] )* only_positive( (x_new - x[N-1])^3 ) +
      ( x[N-1] - x[j] )/( x[N] - x[N-1] ) * only_positive( (x_new - x[N])^3 )
  }
  
  return(value)
}

compute_H <- function(i,j){
  return( compute_basis(x[i], j) )
}
```

## (a)
```{r}
y <- c(0, 1.5, 2, 0.5, 0, -0.5,
       0, 1.5, 3.5, 4.5, 3.5)

Omiga <- matrix(0, nrow = N, ncol = N)

for(j in 1:(N-2))
  for(k in 1:(N-2))
  {
    Omiga[j+2,k+2] = compute_omiga(j, k)
  }
Omiga
```
```{r}
H_matrix <- matrix(nrow = N, ncol = N)
for(i in 1:N)
  for(j in 1:N){
    H_matrix[i, j] <- compute_H(i, j)
  }
H_matrix
```
```{r}
( K_matrix <- solve( t(H_matrix) ) %*% Omiga %*% solve( H_matrix ) )
```

## (b)
```{r}
eigen(K_matrix)
svd(K_matrix)
K <-( K_matrix + t(K_matrix) )/2
```

# Problem 5

## (a)
```{r}
x <- seq(0,1,by=0.1)
N <- length(x)
B <- as.matrix( cbind(1,x) )

compute_K_lambda <- function(lambda, x0, xi){
  value <- dnorm( (x0 - xi)/lambda )
  
  return(value)
}

compute_W <- function(x0, lambda){
  value <- numeric(N)
  for(i in 1:N){
    value[i] <- compute_K_lambda(lambda, x0, x[i])
  }
  
  return(diag(value))
}

compute_I <- function(lambda, x0){
  W <- compute_W(x0, lambda)
  
  part1 <- matrix(c(1,x0), nrow = 1)
  part2 <- solve( t(B) %*% W %*% B)
  part3 <- t(B) %*% W
  
  value <- part1 %*% part2 %*% part3
  
  return(value)
}

get_trace_L <- function(lambda, target = 0){
  L <- apply(as.matrix( x ), 1, compute_I, lambda = lambda)
  
  return( tr(L) - target )
}
```

This is the plot of effective degree of freedom v.s. $log(\lambda)$:
```{r}
lambda <- seq(0.035,10,length.out = 2000)
trace_I <- apply(as.matrix( lambda ), 1,get_trace_L)
plot(log(lambda), trace_I, ty = "l", ylab = "Degree of Freedom",
     xlab = expression( paste( "log(",~lambda,")" ) ) 
     )
```

Find out the value of alpha by solving the root:
```{r}
df <- c(2.5, 3, 4, 5)
lambda_roots <- numeric(length(df))

for(i in 1:length(df)){
  lambda_roots[i] <- uniroot(function(x) get_trace_L(x, target = df[i]), interval = c(0.05,4))$root
}

lambda_roots
```