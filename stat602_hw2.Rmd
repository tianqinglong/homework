---
title: "stat602_hw2"
output:
  pdf_document: default
  html_document:
    df_print: paged
  word_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyr)
library(caret)
library(readxl)
library(glmnet)
library(lava)
library(GGally)
library(dplyr)
```

# 4.3

## First we read in the data set and remove the irrelavant columns.
```{r}
house <- read_excel("~/work/homework/AmesHousingData.xlsx")
house <- data.frame(Price = house$Price, Size = house$Size,
                    Fireplace = house$Fireplace,
                    Basementbath = house$`Bsmt Bath`, Land = house$Land, intersect = 1)
head(house)
```

## Now we need a matrix of all the possible regressor combinations.
```{r}
regMat <- expand.grid(c(TRUE,FALSE), c(TRUE,FALSE),
                      c(TRUE,FALSE), c(TRUE,FALSE))
regMat <- cbind(TRUE,regMat)

head(regMat)
```

## Then we construct a formula so that from each row of regMat we can determine the the regressor combinations.
```{r}
formu <- function(vec)
{
  vec <- as.matrix(vec)
  regressors <- c("intersect","Size", "Fireplace", "Basementbath", "Land")
  out <- as.formula(paste(c("Price ~ 0", regressors[vec]), collapse=" + "))
  
  return(out)
}

formu(regMat[1,])
```

## Cross-validation using **caret** package

We will repeat the 8-fold crossval times.
```{r, warning=FALSE}
ctrl <- trainControl(method = "repeatedcv", repeats = 5, number = 8)

RMSE <- numeric(16)
for(i in 1:length(RMSE)){
  model_caret <- train(formu(regMat[i,]), data = house, trControl = ctrl, method = "lm")
  RMSE[i] = model_caret$results[,2]
}
RMSE
order(RMSE)
regMat
```
We can see that the full model has the smallest mean squared prediction error.

# 4.4

First read in the dataset:
```{r}
glass <- read.csv("~/work/homework/glass.data", header=FALSE)
names(glass) <- c("Id", "RI", "Na", "Mg", "Al", "Si", "K", "Ca", "Ba", "Fe", "Type")
glass_sub <- subset(glass, Type %in% c(1,2))
glass_sub$Type <- as.factor(glass_sub$Type)
```

## (a)

The best number of neighbors for this prediction task is $1$.
```{r}
ctrl <- trainControl(method="repeatedcv",repeats = 10, number = 10)
knnFit <- train(Type ~ . - Id, data = glass_sub, method = "knn", trControl = ctrl, preProcess = c("center","scale"), tuneGrid = expand.grid(k = 1:20) , tuneLength = 30)
plot(knnFit)
```

## (b)

See hand-written solution.

# 5.1

## (a)
```{r}
X <- c(2, 4, 3, 5, 1, 4, 3, 4, 2, 3, 7, 5, 6, 4, 4, 2, 5, 1, 2, 4)
X <- matrix(X, nrow = 5)

qrObj <- qr(X)
svdObj <- svd(X)
```

The Q matrix and R matrix are:
```{r}
qr.Q(qrObj)
qr.R(qrObj)
```
The colums of Q matrix give the bases for $C(\textbf{X})$.

The SVD results are:
```{r}
svdObj
```
The U has columns spanning $C(\textbf{X})$.

## (b)

The eigen decomposition of $X^{'}X$ is:
```{r}
(V <- svdObj$v)
(D_sq <- diag(svdObj$d) %*% diag(svdObj$d))
```

The eigen decomposition of $XX^{'}$ is:
```{r}
(U <- svdObj$u)
(D_sq <- diag(svdObj$d) %*% diag(svdObj$d))
```

## (c)

```{r, include=FALSE}
best_approx <- function(X_matrix, rank)
{
  x_svd <- svd(X_matrix)
  value <- as.matrix( x_svd$u[, 1:rank] ) %*% 
    diag(x = x_svd$d[1:rank], nrow = rank ) %*%
    as.matrix( t( x_svd$v[, 1:rank] ) )
  
  return(value)
}
```

rank = 1
```{r}
( rank1approx <- best_approx(X, 1) )
```

rank = 2
```{r}
( rank2approx <- best_approx(X, 2) )
```

## (d)

First center the matrix:
```{r}
X_center <- scale(X, center = TRUE, scale = FALSE)

X_center_scale <- scale(X, center = T, scale = T)
```

```{r}
( svdObj2 <- svd(X_center) )
( svdObj3 <- svd(X_center_scale) )
```

The principal component directions:
```{r}
svdObj2$v
svdObj3$v
```

The "loadings" of the first principal component:
```{r}
svdObj2$v[,1]
svdObj3$v[,1]
```

The principal components:
```{r}
X_center %*% svdObj2$v
X_center_scale %*% svdObj3$v
```

## (e)

The rank = 1 approximation:
```{r}
( rank1approx_center <- best_approx(X_center, 1) )
( rank1approx_center_scale <- best_approx(X_center_scale, 1) )
```

The rank = 2 approximation:
```{r}
( rank1approx_center <- best_approx(X_center, 2) )
( rank1approx_center_scale <- best_approx(X_center_scale, 2) )
```

## (f)

```{r}
cov_matrix_center <- 1/5 * t(X_center) %*% X_center
cov_matrix_center_scale <- 1/5 * t(X_center_scale) %*% X_center_scale

( eigenX <- eigen( cov_matrix_center ) )
( best_approx(cov_matrix_center, 1) )
( best_approx(cov_matrix_center, 2) )

( eigenX <- eigen( cov_matrix_center_scale ) )
( best_approx(cov_matrix_center_scale, 1) )
( best_approx(cov_matrix_center_scale, 2) )
```

# Problem 2

```{r, include= FALSE}
data2 <- read_excel("~/work/homework/Problem3.4Data.xlsx")
```

Making up a matrix:
```{r}
mother_wavelet <- function(x){
  
  value = ( x > 0 && x <= 1/2 ) - ( x > 1/2 && x <= 1)
  
  return(value)
}

haar_basis <- function(x){
  index <- 1;
  row <- numeric(16)
  for(m in 0:3)
    for( j in 0:( 2^m - 1 ) ){
      row[index] = sqrt(2^m) * mother_wavelet( (2^m) * ( x - j / (2^m) ) )
      index = index + 1
    }
  
  return(row)
}

x_trans <- ( data2$x - min(data2$x) )/( max(data2$x) - min(data2$x) )
X_h <- t( apply(as.matrix(x_trans), 1, haar_basis) )
```

## (a)

```{r}
( fit_ols <- lm(y~X_h, data = data2) )
plot(x = data2$x, y = data2$y, ylab = "Y", xlab = "X",
     pch = 1, cex = 0.5)
lines(x = sort( data2$x ), y = fit_ols$fitted.values[order(data2$x)], pch = 16,
       col = "red", cex = 0.5)
legend("topleft", legend = c("Observed", "OLS Fitted"), col = c("black", "red"),
       pch = c(1, 16))
```

## (b)

```{r, warning = FALSE}
y_centered <- data2$y - mean( data2$y )

get_lambda <- function(M)
{
  fit_lasso <- glmnet(X_h, y_centered, standardize = T, pmax = M,nlambda = 20000)
  return( min(fit_lasso$lambda) )
}

get_predicted <- function(lambda)
{
  fit_lasso <- glmnet(X_h, y_centered, standardize = T, lambda = lambda)
  value = predict(fit_lasso, X_h)
  
  return(value)
}

get_coefficient_vector <- function(lambda){
  fit_lasso <- glmnet(X_h, y_centered, standardize = T, lambda = lambda)
  return(fit_lasso$beta)
}

M <- c(2,4,8)
lambda <- apply(as.matrix(M), 1, get_lambda)
value <- apply(as.matrix(lambda), 1, get_predicted) + mean(data2$y)

( coef <- apply( as.matrix(lambda), 1, get_coefficient_vector) )

plot(x = data2$x, y = data2$y, ylab = "Y", xlab = "X",
     pch = 1, cex = 0.5)

points(x = data2$x, y = value[,1], cex = 0.75, pch = 2, col = 2)
points(x = data2$x, y = value[,2], cex = 0.75, pch = 3, col = 3)
points(x = data2$x, y = value[,3], cex = 0.75, pch = 4, col = 4)

legend("topright", legend = c("M = 2", "M = 4", "M = 8"), col = c(2,3,4), pch = c(2,3,4))
```

# Problem 3

```{r, include= FALSE}
data2 <- read_excel("~/work/homework/Problem3.4Data.xlsx")
```

```{r}
knot <- c(0, .1, .3, .5, .7, .9, 1)
K <- length(knot)

only_positive <- function(x){
  value <- ifelse(x >= 0, x, 0)
  
  return(value)
}

make_row <- function(x){
  row <- numeric(K)
  row[1] = 1
  row[2] = x
  
  for(j in 1:(K - 2)){
    value = only_positive( ( x - knot[j] )^3 )
    -( knot[K] - knot[j] )/( knot[K] - knot[K-1] ) * only_positive( (x - knot[K -1])^3 )
    +( knot[K-1] - knot[j] )/( knot[K] - knot[K-1] ) * only_positive( (x - knot[K])^3 )
    
    row[j+2] = value
  }
  
  return(row)
}

x_trans <- ( data2$x - min(data2$x) )/( max(data2$x) - min(data2$x) )
X_h_3 <- t( apply(as.matrix(x_trans), 1, make_row) )
```

Then to find $\hat{\beta}^{OLS}$:
```{r}
fit_ols <- lm(data2$y ~ 0 + X_h_3)
fit_ols$coefficients

x_plot <- seq(from = min( data2$x ), to = max( data2$x), length.out = 500)
x_plot_trans <- ( x_plot - min(data2$x) )/( max(data2$x) - min(data2$x) )
x_plot_h <- t( apply(as.matrix(x_plot_trans), 1, make_row) )
y_plot <- x_plot_h %*% fit_ols$coefficients

plot(x = data2$x, y = data2$y, pch = 16, cex = 0.5,
     ylab = "Y", xlab = "X", main = "Natural Cubic Regression Spline")
lines(x = x_plot, y = y_plot, col = "red")
```

# Problem 4

```{r, include=FALSE}
x <- seq(0,1,by=0.1)
N <- length(x)

compute_omiga <- function(j,k)
{
  value = 6 * (x[N-1] - x[k])^2 * (2*x[N-1] + x[k] - 3*x[j]) +
    12 * (x[N-1] - x[k]) * (x[N-1] - x[j]) *(x[N] - x[N-1])
  
  return(value)
}

compute_basis <- function(x_new, j){
  if(j == 1){
    value = 1
  }else if(j == 2){
    value = x_new
  }else{
    j = j-2
    value = only_positive( (x_new - x[j])^3 ) -
      ( x[N] - x[j] )/( x[N] - x[N-1] )* only_positive( (x_new - x[N-1])^3 ) +
      ( x[N-1] - x[j] )/( x[N] - x[N-1] ) * only_positive( (x_new - x[N])^3 )
  }
  
  return(value)
}

compute_H <- function(i,j){
  return( compute_basis(x[i], j) )
}
```

## (a)
```{r}
y <- c(0, 1.5, 2, 0.5, 0, -0.5,
       0, 1.5, 3.5, 4.5, 3.5)

Omiga <- matrix(0, nrow = N, ncol = N)

for( j in 1:(N-2) )
  for( k in j:(N-2)){
    if(k == j){
      Omiga[j+2, k+2] = 12 * (x[N-1] - x[j])^2 * (x[N] - x[j])
    } else {
      Omiga[j+2, k+2] = 6 * ( x[N-1] - x[k] )^2 * ( 2*x[N-1] + x[k] - 3*x[j] ) +
        12 * ( x[N-1] - x[k] ) * ( x[N-1] - x[j] ) * ( x[N] - x[N-1] )
      Omiga[k+2, j+2] = Omiga[j+2, k+2]
    }
  }
```
```{r}
H_matrix <- matrix(nrow = N, ncol = N)
for(i in 1:N)
  for(j in 1:N){
    H_matrix[i, j] <- compute_H(i, j)
  }
H_matrix
```
```{r}
K_matrix <- solve( t(H_matrix) ) %*% Omiga %*% solve( H_matrix )
K_matrix <- ( K_matrix + t(K_matrix) )/2
```

## (b)
```{r}
K_eigen <- eigen(K_matrix); K_eigen$values

reshape2::melt(K_eigen$vectors) %>% mutate(Var2 = as.factor(Var2)) %>%
  filter(Var2 %in% 1:9) %>% ggplot() + geom_line(aes(x = Var1, y = value, col = Var2))
```

A large eigen value of $K$ corresponds to a small eigen value of $S_{\lambda}$. From the plot below, we can draw the conclusion that the ```Y[c(2,3,6,9)]``` will get suppressed most in the smoothing.
```{r}
reshape2::melt(K_eigen$vectors) %>%
  mutate(Var2 = as.factor(Var2)) %>%
  filter( Var2 %in% c(1:2, 8:9) ) %>%
  ggplot(aes( x = Var1, y = value, col = Var2)) + geom_line(aes( linetype = Var2 )) +geom_point(aes(pch = Var2))
```

## (c)
```{r}
compute_df <- function(lambda, eta_array, target = 0){
  eta_array[10:11] = 0
  value <- sum( 1/(1 + lambda * eta_array) )
  
  return( value-target )
}

lambda <- as.matrix( seq(from = 0.001, to = 0.5, by = 0.0001) )
df <- apply(lambda, 1, compute_df, eta_array = K_eigen$values)

plot(x = lambda, y = df, type = "l")
```
```{r}
df <- c(2.5, 3, 4, 5)
lambda_roots_4 <- numeric(length(df))

for(i in 1:length(df)){
  lambda_roots_4[i] <- uniroot(function(x) compute_df(x, eta_array = K_eigen$values, target = df[i]), interval = c(0.0001, 2))$root
}
lambda_roots_4
```

# Problem 5

## (a)
```{r}
x <- seq(0,1,by=0.1)
N <- length(x)
B <- as.matrix( cbind(1,x) )

compute_K_lambda <- function(lambda, x0, xi){
  value <- dnorm( (x0 - xi)/lambda )
  
  return(value)
}

compute_W <- function(x0, lambda){
  value <- numeric(N)
  for(i in 1:N){
    value[i] <- compute_K_lambda(lambda, x0, x[i])
  }
  
  return(diag(value))
}

compute_I <- function(lambda, x0){
  W <- compute_W(x0, lambda)
  
  part1 <- matrix(c(1,x0), nrow = 1)
  part2 <- solve( t(B) %*% W %*% B)
  part3 <- t(B) %*% W
  
  value <- part1 %*% part2 %*% part3
  
  return(value)
}

get_trace_L <- function(lambda, target = 0){
  L <- apply(as.matrix( x ), 1, compute_I, lambda = lambda)
  
  return( tr(L) - target )
}
```

This is the plot of effective degree of freedom v.s. $log(\lambda)$:
```{r}
lambda <- seq(0.035,10,length.out = 2000)
trace_I <- apply(as.matrix( lambda ), 1,get_trace_L)
plot(log(lambda), trace_I, ty = "l", ylab = "Degree of Freedom",
     xlab = expression( paste( "log(",~lambda,")" ) ) 
     )
```

Find out the value of alpha by solving the root:
```{r}
df <- c(2.5, 3, 4, 5)
lambda_roots <- numeric(length(df))

for(i in 1:length(df)){
  lambda_roots[i] <- uniroot(function(x) get_trace_L(x, target = df[i]), interval = c(0.05,4))$root
}

lambda_roots
```

## (b)

```{r}
lambda5 <- lambda_roots[3]
lambda4 <- lambda_roots_4[3]

L_lambda5 <- t( apply(as.matrix( x ), 1, compute_I, lambda = lambda5) )
S_lambda4 <- solve( diag(1, nrow = 11) + lambda4*K_matrix )
```

The matrix difference is:
```{r}
L_lambda5 - S_lambda4
```

The plots are:
```{r}
par(mfrow = c(2,2))

for(i in c(1,3,5)){
  plot(x = 1:11, y = S_lambda4[i, ], xlab = "Column Index", col = 1,
     main = paste("Row", i, " of the Smoothing Matrix"), pch = 3, ylab = "Value")
  lines(x = 1:11, y = S_lambda4[i, ], col = 1, lty = 1)
  
  points(x = 1:11, y = L_lambda5[i, ], pch = 2, col = 2)
  lines(x = 1:11, y = L_lambda5[i, ], col = 2, lty = 2)
  
  legend("topright", legend = c("S Matrix", "L Matrix"), pch = c(3, 2), col = c(1, 2), lty = c(1, 2))
}

```
From the plots above, the two smoothing matrices are close to each other when the effective degree of freedom is the same.

# 6

## (a)
```{r, include=FALSE}
x <- seq(0, 1, by = 0.1)
y <- c(0, 1.5, 2, 0.5, 0, -0.5, 0, 1.5, 3.5, 4.5, 3.5)
```

The largest effective degrees of freedom I can get for the locally weighted linear regression smoother with tricube kernel is about 4.72. If I try to go over it, the effective degrees of freedom will jump to 11, which means the fitted values are the same as the observed values.
```{r, warning=FALSE}
lambda_df5 <- uniroot(function(lambda) smooth.spline(x, y, lambda = lambda)$df-5,
                      interval = c(0.0001, 1) )$root
lambda_df9 <- uniroot(function(lambda) smooth.spline(x, y, lambda = lambda)$df-9,
                      interval = c(0.000001, 1))$root
cubic_smoothing_5 <- smooth.spline(x, y, lambda = lambda_df5)
cubic_smoothing_9 <- smooth.spline(x, y, lambda = lambda_df9)

tricube_5 <- loess(y~x, degree = 1, enp.target = 5)
( tricube_9 <- loess(y~x, degree = 1, enp.target = 9) )
```

This is the plot for approximately 5 effective degrees of freedom:
```{r}
plot(x, y, pch = 2, col = 2)
points(x, cubic_smoothing_5$y, ylab = "Y", xlab = "x")
points(x, tricube_5$fitted, pch = 3, col = 3)

lines(x, cubic_smoothing_5$y)
lines(x, tricube_5$fitted, lty = 3, col = 3)
lines(x, y, lty = 2, col = 2)
legend("bottomright", legend = c("Cubic Smoothing", "Observed Values", "Tricube Kernel"),
       pch = c(1, 2, 3), col = c(1, 2, 3), lty = c(1, 2, 3))
```

This is the plot for approximately 9 effective degrees of freedom:
```{r}
plot(x, y, pch = 2, col = 2)
points(x, cubic_smoothing_9$y, ylab = "Y", xlab = "x")
points(x, tricube_9$fitted, pch = 3, col = 3)

lines(x, cubic_smoothing_9$y)
lines(x, tricube_9$fitted, lty = 3, col = 3)
lines(x, y, lty = 2, col = 2)
legend("bottomright", legend = c("Cubic Smoothing", "Observed Values", "Tricube Kernel"),
       pch = c(1, 2, 3), col = c(1, 2, 3), lty = c(1, 2, 3))
```
We can see that, the fitted value from locally weighted linear regression is the same as the observed one. The reason for that is explained above. Maybe it is caused by some numerical issues.

# Problem 7
```{r, include = FALSE}
wine <- read.csv("~/work/homework/winequality-white.csv", sep=";")
load("winefit.Rdata")
```

```{r, eval=FALSE}
wineknn<- train(y=wine$quality,
                x=wine[,-12],
                method="knn",
                preProcess=c("center","scale"),
                trControl=trainControl(method="repeatedcv",repeats=100,number=10))

winenet<- train(y=wine$quality,
                x=wine[,-12],
                method="glmnet",
                preProcess=c("center","scale"),
                trControl=trainControl(method="repeatedcv",repeats=100,number=10))

winepcr<- train(y=wine$quality,
                x=wine[,-12],
                method="pcr",
                preProcess=c("center","scale"),
                trControl=trainControl(method="repeatedcv",repeats=100,number=10))

winepls<- train(y=wine$quality,
                x=wine[,-12],
                method="pls",
                preProcess=c("center","scale"),
                trControl=trainControl(method="repeatedcv",repeats=100,number=10))

winemars<- train(y=wine$quality,
                 x=wine[,-12],
                 method="earth",
                 preProcess=c("center","scale"),
                 trControl=trainControl(method="repeatedcv",repeats=100,number=10))

wineols<- lm(quality~.,data=wine)
```
```{r}
winepred<- sapply(list(wineknn,winenet,winepcr,winepls,winemars,wineols),predict)
colnames(winepred)<- c("kNN","Net","PCR","PLS","MARS","OLS")

res<- data.frame(cbind(Quality=wine$quality,winepred))

ggpairs(res)

round(cor(res),digits=2)
```

# Problem 8

## (a)
```{r}
x <- seq(0, 1, by = 0.1)
y <- c(0, 1.5, 2, 0.5, 0, -0.5, 0, 1.5, 3.5, 4.5, 3.5)

x_std <- scale(x, center = T, scale = T)
y_ctr <- scale(y, center = T, scale = FALSE)

K <- as.matrix( -1/2 * dist(x_std)^2 ) %>% exp

G <- matrix(nrow = 11, ncol = 11)
for(i in 1:11)
  for(j in 1:11){
    G[i, j] = K[i, j] - colMeans(K)[i] - rowMeans(K)[j] + mean(K)
  }
G
```

## (c)
```{r}
G_eigen <- eigen(G)

compute_T <- function(z, x){
  return( exp(-1/2 * ( x - z )^2 ) )
}

compute_S <- function(z, i){
  T_xi <- compute_T( z, x_std[i] )
  
  M <- 0
  for(j in 1:11){
    M <- M + compute_T( z, x_std[j] )
  }
  M <- M/11
  
  return(T_xi + M)
}
```

The function is becoming closer to 0 with decreasing eigenvalues, but the smallest eigen value is an outlier.
```{r}
X_vector <- NULL
Y_vector <- NULL
Eigen_vector <- NULL

for(i in 1:11){
  eigen_value <- G_eigen$values[i]
  
  
  x_plot <- seq(from = min( x_std ), to = max( x_std ), by = 0.01)
  y_plot <- numeric( length(x_plot) )
  
  Eigen_vector <- c( Eigen_vector, rep(eigen_value, times = length(x_plot)) )
  X_vector <- c(X_vector, x_plot)
  
  for(ii in 1:length(x_plot) ){
    y_value <- 0
    for(jj in 1:11){
      y_value <- y_value + G_eigen$vectors[,i][jj] * compute_S( x_plot[ii], jj )
    }
    y_plot[ii] <- y_value
  }
  
  Y_vector <- c(Y_vector, y_plot)
}

X_vector <- ( sd(x) * X_vector + mean(x) )

plot_frame <- data.frame(X = X_vector, Y = Y_vector, Lambda = Eigen_vector)

ggplot(data = plot_frame, mapping = aes(x = X, y = Y) )+ geom_line() + facet_wrap(~Lambda)
```

## (d)

The eigen decomposition is equivalent to SVD in this problem. The following is the plot:
```{r}
compute_S065 <- function(y)
{
  value <- compute_T(y, 0.65)
  for(i in 1:11){
    value <- value - 1/11 * compute_T( y, x_std[i] )
  }
  
  return(value)
}

for(i in 1:length(x_plot))
{
  y_plot[i] <- compute_S065(x_plot[i])
}

kernel_inner <- function(x, y){
  return( exp(-(x - y)^2/2) )
}

S_65_S_xj <- function(j){
  value <- kernel_inner(0.65, x_std[j])
  
  part2 <- 0
  for(i in 1:11){
    part2 <- part2 + kernel_inner(x_std[i], 0.65) + kernel_inner(x_std[i], x_std[j])
  }
  part2 <- -1/11 * part2
  
  part3 <- 0
  dict <- expand.grid(x_std, x_std)
  for(i in 1:121){
    part3 <- part3 + kernel_inner(dict[i,1], dict[i,2])
  }
  part3 <- part3 / 121
  
  return(value + part2 + part3)
}

compute_s_weight <- function(y, j)
{
  weight <- rowSums(G_eigen$vectors)
  value <- weight[j] * S_65_S_xj(j) * compute_S(y, j)
  return(value)
}

final_function <- function(y){
  value = 0;
  for(i in 1:11){
    value = value + compute_s_weight(y, i)
  }
  return(value)
}

y_proj <- numeric(length(x_plot))
for(i in 1:length(x_plot)){
  y_proj <- final_function(x_plot)
}

data.frame(X = sd(x) * x_plot+mean(x), Original = y_plot, Projection = y_proj) %>% reshape2::melt(id.vars = "X") %>% ggplot() +geom_line(aes(x = X, y = value, col = variable))
```